y
dat<-as.data.frame(cbind(x,y))
View(dat)
boxplot(x,y,xlab= 'x Axis', ylab = 'y Axis', col='blue')
ggplot()+geom_point(data=dat, aes(x=x,y=y),size=3, color="blue")+ggtitle("Title")+labs(y= "y axis ", x = "x axis ")
ggplot()+geom_point(data=dat, size=3, color="blue")+ggtitle("Title")+labs(y= "y axis ", x = "x axis ")
library(ggplot2)
library(plyr)
library(lattice)
library(graphics)
# data
x <- c(85,70,68,79,82,88)
y <- c(67,51,55,63,67,72)
clsample <- data.frame(x,y)
# k-means clustering
km <- kmeans(cldata, centers = 2, nstart = 10)
# k-means clustering
km <- kmeans(clsample, centers = 2, nstart = 10)
km
# Visualizing clusters
plot(cldata[km$cluster == 1,], col = "red",xlim = c(min(cldata[,1]),max(cldata[,1])),ylim = c(min(cldata[,2]),max(cldata[,2])))
# Visualizing clusters
plot(clsample1[km$cluster == 1,], col = "red",xlim = c(min(cldata[,1]),max(cldata[,1])),ylim = c(min(cldata[,2]),max(cldata[,2])))
# Visualizing clusters
plot(clsample[km$cluster == 1,], col = "red",xlim = c(min(cldata[,1]),max(cldata[,1])),ylim = c(min(cldata[,2]),max(cldata[,2])))
# Visualizing clusters
plot(clsample[km$cluster == 1,], col = "red",xlim = c(min(clsample[,1]),max(clsample[,1])),ylim = c(min(clsample[,2]),max(clsample[,2])))
points(cldata[km$cluster == 2,],col = "blue")
points(clsample[km$cluster == 2,],col = "blue")
plot(x,y)
km$centers # centroid of each cluster
poi<-km$centers # centroid of each cluster
poi
# Visualizing clusters
plot(clsample[km$cluster == 1,], col = "red",xlim = c(min(clsample[,1]),max(clsample[,1])),ylim = c(min(clsample[,2]),max(clsample[,2])))
points(clsample[km$cluster == 2,],col = "blue")
points(poi,col="green")
km$cluster # clustering vector
km$withinss
count<-c(9,25,15,2,14,25,24,47)
speed<-c(2,3,5,9,14,24,29,34)
fw<-data.frame(count,speed)
fw
cor(count,speed)
names<-c("Taw","Torridge","Ouse","Exe","Lyn","Brook","Ditch","Fal")
rownames(fw)=names
fw
fw.lm<-lm(count~speed,data=fw)
summary(fw.lm)
names(fw.lm)
fw.lm$coefficients
#gives slope and intercept
newpred<-fitted(fw.lm)
newpred
#plotting the x, y values
plot(fw$speed,fw$count,col='red')
abline(coef(fw.lm),lty=1,col="blue")
#obtaining confidence interval
confint(fw.lm)
#obtaining confidence interval
confint(fw.lm)
confint(fw.lm,parm=c('(Intercept)','speed'))
#fitted values
fitted(fw.lm)
residuals(fw.lm)
newpred
#plotting the fitted line
coef(fw.lm)
#passing residuals
plot(fw.lm,which=1)
#Loading data
data(iris)
# Structure
str(iris)
# Installing Packages
install.packages("e1071")
install.packages("caTools")
install.packages("caret")
# Loading package
library(e1071)
library(caTools)
library(caret)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.7)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
# Feature Scaling
train_scale <- scale(train_cl[, 1:4])
test_scale <- scale(test_cl[, 1:4])
# Fitting Naive Bayes Model
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ ., data = train_cl)
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test_cl)
classifier_cl
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test_cl)
# Confusion Matrix
cm <- table(test_cl$Species, y_pred)
cm
# Model Evauation
confusionMatrix(cm)
library(e1071)
v
v <- c(1,2,3)
v
library("scatterplot3d") # load
install.packages("scatterplot3d")
library("scatterplot3d") # load
data(iris)
head(iris)
colors <- c("#999999", "#E69F00", "#56B4E9")
colors <- colors[as.numeric(iris$Species)]
scatterplot3d(iris[,1:3],
main="3D Scatter Plot",
xlab = "Sepal Length (cm)",
ylab = "Sepal Width (cm)",
zlab = "Petal Length (cm)",pch = 16,color = colors, type="h",
grid=TRUE, box=TRUE)
install.packages("rpart.plot")
install.packages("rpart")
library("rpart")
library("rpart.plot")
play <- c('yes', 'no','yes', 'no', 'yes', 'yes', 'yes', 'yes','yes', 'no')
outlook <- c('rainy', 'rainy', 'overcast', 'sunny','rainy','sunny', 'rainy', 'sunny', 'overcast','sunny')
temperature <- c('cool','cool', 'hot', 'mild', 'cool', 'cool', 'cool','hot', 'mild','mild')
humidity <- c('normal', 'normal', 'high', 'high','normal', 'normal','normal', 'normal', 'high', 'high' )
wind <- c('FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE','FALSE', 'FALSE','FALSE', 'TRUE','TRUE')
play_decision <- data.frame(play, outlook, temperature, humidity, wind)
play_decision
str(play_decision)
summary(play_decision)
fit=rpart(play~outlook+temperature+humidity+wind,method="class",data=play_decision,control=rpart.control(minsplit=1),
parms=list(split='information'))
fit
str(fit)
rpart.plot(fit,type=4,extra=1)
rpart.plot(fit,type=4,extra=2,clip.right.labs=FALSE,varlen=0,faclen=0)
library("gcookbook")
str(heightweight)
boxplot(heightIn~ageYear,data = heightweight ,xlab = "x", ylab = "y", main = "boxplot")
library(cluster)
library(ggplot2)
library(plyr)
library(lattice)
library(plyr)
library(graphics)
x<-c(185,170,168,179,182,188)
y<-c(72,56,60,68,72,77)
clsample<-data.frame(x,y)
wss<-vector(mode="numeric",length=6)
wss<-(nrow(clsample1)-1)*sum(apply(clsample1,2,var))
clsample1=clsample
wss<-(nrow(clsample1)-1)*sum(apply(clsample1,2,var))
wss
for(i in 1:6) {
wss[i]=sum(kmeans(cldata,centers=i,nstart=25)$withinss)
}
cldata<-clsample1[,1:2]
for(i in 1:6) {
wss[i]=sum(kmeans(cldata,centers=i,nstart=25)$withinss)
}
nrow(cldata)
for(i in 1:5) {
wss[i]=sum(kmeans(cldata,centers=i,nstart=25)$withinss)
}
plot(1:5,wss,type='b')
#install.packages("e1071")
library(e1071)
df = read.csv("bayes.csv")
str(df)
class(df)
df
testset = data.frame(Age="<=30",Income="High",JobSatisfaction= "Yes",Desire="Fair",Enrolls="")
testset
df = rbind(df,testset)
df
df$Enrolls <- factor(df$Enrolls, levels = c("No","Yes"), labels = c("False", "True"))
df
train = as.data.frame(df[1:14,])
test = as.data.frame(df[15,])
train
test
Bm = naiveBayes(Enrolls ~ Age+Income+JobSatisfaction+Desire,train)
Bm
res = predict(Bm,test)
res
?predict
modell = naiveBayes(Enrolls ~ ., train, laplace=.01)
modell
results1<-predict(modell,test)
results1
Bm
res
bajaj <- c(26,23,33,6,3,4,20,2)
honda <- c(4,5,12,9,15,10,8,22)
speed1 <- c(3,4,4,5,6,7,7,9)
speed2 <- c(5,6,2,8,9,10,12,14)
spd < cbind(speed1,speed2)
spd <- cbind(speed1,speed2)
ivert <- cbind(bajaj,honda)
matplot(spd,ivert,type='b')
matplot(spd,ivert,type='b',lty=2:3,xlab='Speed',ylab='Vehicle'))
matplot(spd,ivert,type='b',lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1,pch=1:2,lty=2:3)
matplot(spd,ivert,type='b',pch=1:2,col=1,lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1,pch=1:2,lty=2:3)
matplot(spd,ivert,type='b',col=1,lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1,pch=1:2,lty=2:3)
matplot(spd,ivert,type='b',pch=1:2,col=1,lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1,pch=1:2,lty=2:3)
matplot(spd,ivert,type='b',pch=15:16,col=1,lty=2:3,xlab='Speed',ylab='Vehicle')
matplot(spd,ivert,type='b',pch=10:11,col=1,lty=2:3,xlab='Speed',ylab='Vehicle')
matplot(spd,ivert,type='b',pch=2:3,col=1,lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1,pch=2:3,lty=2:3)
matplot(spd,ivert,type='b',pch=2:3,col=3:4,lty=2:3,xlab='Speed',ylab='Vehicle')
matplot(spd,ivert,type='b',pch=2:3,col=5:6,lty=2:3,xlab='Speed',ylab='Vehicle')
matplot(spd,ivert,type='b',pch=2:3,col=1:2,lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1:2,pch=2:3,lty=2:3)
matplot(spd,ivert,type='a',pch=2:3,col=1:2,lty=2:3,xlab='Speed',ylab='Vehicle')
matplot(spd,ivert,type='c',pch=2:3,col=1:2,lty=2:3,xlab='Speed',ylab='Vehicle')
matplot(spd,ivert,type='b',pch=2:3,col=1:2,lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1:2,pch=2:3,lty=2:3)
library("rpart")
library("rpart.plot")
age<-c('youth','youth','middle-age','senior','senior','senior','middle-age','youth','youth','senior','youth','middle-age','middle-age','senior')
income<-c('high','high','high','medium','low','low','low','medium','low','medium','medium','medium','high','medium')
student<-c('no','no','no','no','yes','yes','yes','no','yes','yes','yes','no','yes','no')
credit<-c('fair','excellent','fair','fair','fair','excellent','excellent','fair','fair','fair','excellent','excellent','fair','excellent')
classbuys<-c('no','no','yes','yes','yes','no','yes','no','yes','yes','yes','yes','yes','no')
cb<-cbind(age,income,student,credit,classbuys)
df=as.data.frame(cb)
df
fit=rpart(classbuys ~ age + income + student + credit, method="class",
data=df, control=rpart.control(minsplit=1), parms=list(split='information'))
fit
str(fit)
rpart.plot(fit,type=4,extra=2,clip.right.labs=FALSE)
newdata<-data.frame(age='senior',income='high',student='no','credit'='fair')
newdata
predict(fit,newdata=newdata,type="prob")
predict(fit,newdata=newdata,type="class")
newdata<-data.frame(age='senior',income='high',student='no',credit='fair')
newdata
predict(fit,newdata=newdata,type="prob")
predict(fit,newdata=newdata,type="class")
fit
str(fit)
rpart.plot(fit,type=4,extra=2,clip.right.labs=FALSE)
rpart.plot(fit,type=3,extra=2,clip.right.labs=FALSE)
rpart.plot(fit,type=3,extra=2,clip.right.labs=FALSE)
rpart.plot(fit,type=4,extra=2,clip.right.labs=FALSE)
rpart.plot(fit,type=4,extra=2,clip.right.labs=TRUE)
rpart.plot(fit,type=4,extra=2,clip.right.labs=FALSE)
rpart.plot(fit,type=4,extra=2,clip.right.labs=TRUE)
rpart.plot(fit,type=4,extra=2,clip.right.labs=FALSE)
fit
rpart.plot(fit, extra= 106)
rpart.plot(fit,type=4,extra=2,clip.right.labs=FALSE)
rpart.plot(fit, extra= 106)
predict(fit,newdata=newdata,type="class")
matplot(spd,ivert,type='b',pch=2:3,col=1:2,lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1:2,pch=2:3,lty=2:3)
matplot(spd,ivert,type='b',pch=2:3,col=1:2,lty=2:3,xlab='Speed',ylab='Vehicle')
legend(x = 'topright',legend =c('Bajaj','Honda'),col=1:2,pch=2:3,lty=2:3)
df
#rpart.plot(fit,type=4,extra=2,clip.right.labs=FALSE)
rpart.plot(fit, extra= 106)
predict(fit,newdata=newdata,type="prob")
predict(fit,newdata=newdata,type="class")
#Clean the environment
rm(list = ls())
#Set working directory
setwd("C:/Users/hriti/OneDrive/Documents/Employee-Absenteeism-master")
#Load the librarires
libraries = c("dummies","caret","rpart.plot","plyr","dplyr", "ggplot2","rpart","dplyr","DMwR","randomForest","usdm","corrgram","DataCombine","xlsx")
lapply(X = libraries,require, character.only = TRUE)
rm(libraries)
#Read the csv file
emp_absent = read.csv("sheet.csv",stringsAsFactors = F)
#Check number of rows and columns
dim(emp_absent)
#Observe top 5 rows
head(emp_absent)
#Structure of variables
str(emp_absent)
########
#emp_absent$Work.load.Average.day =as.numeric(as.character(emp_absent$Work.load.Average.day))
#######
#Transform data types
emp_absent$ID = as.factor(as.character(emp_absent$ID))
emp_absent$Reason.for.absence[emp_absent$Reason.for.absence %in% 0] = 20
emp_absent$Reason.for.absence = as.factor(as.character(emp_absent$Reason.for.absence))
emp_absent$Month.of.absence[emp_absent$Month.of.absence %in% 0] = NA
emp_absent$Month.of.absence = as.factor(as.character(emp_absent$Month.of.absence))
emp_absent$Day.of.the.week = as.factor(as.character(emp_absent$Day.of.the.week))
emp_absent$Seasons = as.factor(as.character(emp_absent$Seasons))
emp_absent$Disciplinary.failure = as.factor(as.character(emp_absent$Disciplinary.failure))
emp_absent$Education = as.factor(as.character(emp_absent$Education))
emp_absent$Son = as.factor(as.character(emp_absent$Son))
emp_absent$Social.drinker = as.factor(as.character(emp_absent$Social.drinker))
emp_absent$Social.smoker = as.factor(as.character(emp_absent$Social.smoker))
emp_absent$Pet = as.factor(as.character(emp_absent$Pet))
#Structure of variables
str(emp_absent)
#Make a copy of data
df = emp_absent
########################################MISSING VALUE ANALYSIS########################################
#Get number of missing values
sum(is.na(df))
sapply(df,function(x){sum(is.na(x))})
missing_values = data.frame(sapply(df,function(x){sum(is.na(x))}))
#Get the rownames as new column
missing_values$Variables = row.names(missing_values)
#Reset the row names
row.names(missing_values) = NULL
#Rename the column
names(missing_values)[1] = "Miss_perc"
head(missing_values)
#Calculate missing percentage
missing_values$Miss_perc = ((missing_values$Miss_perc/nrow(emp_absent)) *100)
#Reorder the columns
missing_values = missing_values[,c(2,1)]
head(missing_values)
#Sort the rows according to decreasing missing percentage
missing_values = missing_values[order(-missing_values$Miss_perc),]
head(missing_values)
#Create a bar plot to visualie top 5 missing values
ggplot(data = missing_values[1:5,], aes(x=reorder(Variables, -Miss_perc),y = Miss_perc))+
geom_bar(stat = "identity",fill = "grey")+xlab("Parameter")+
ggtitle("Missing data percentage") + theme_bw()
#Create missing value and impute using mean, median and knn
df = knnImputation(data = df, k = 3)
#Check if any missing values
sum(is.na(df))
###################################
# Saving output result into excel file
write.csv(missing_values, "Missing_perc_R.xlsx", row.names = F)
######################
########################################EXPLORE DISTRIBUTION USING GRAPHS########################################
#Get numerical data
numeric_index = sapply(df, is.numeric)
numeric_index
numeric_data = df[,numeric_index]
#Distribution of factor data using bar plot
bar1 = ggplot(data = df, aes(x = ID)) + geom_bar() + ggtitle("Count of ID") + theme_bw()
bar1
bar2 = ggplot(data = df, aes(x = Reason.for.absence)) + geom_bar() +
ggtitle("Count of Reason for absence") + theme_bw()
bar2
bar3 = ggplot(data = df, aes(x = Month.of.absence)) + geom_bar() + ggtitle("Count of Month") + theme_bw()
bar3
bar4 = ggplot(data = df, aes(x = Disciplinary.failure)) + geom_bar() +
ggtitle("Count of Disciplinary failure") + theme_bw()
bar4
bar5 = ggplot(data = df, aes(x = Education)) + geom_bar() + ggtitle("Count of Education") + theme_bw()
#(high school (1), graduate (2), postgraduate (3), master and doctor (4)
bar5
bar6 = ggplot(data = df, aes(x = Son)) + geom_bar() + ggtitle("Count of Son") + theme_bw()
bar6
bar7 = ggplot(data = df, aes(x = Social.smoker)) + geom_bar() +
ggtitle("Count of Social smoker") + theme_bw()
bar7
##### show all #########
gridExtra::grid.arrange(bar1,bar2,bar3,bar4,ncol=2)
gridExtra::grid.arrange(bar5,bar6,bar7,ncol=2)
hist1 = ggplot(data = numeric_data, aes(x =Transportation.expense)) +
ggtitle("Transportation.expense") + geom_histogram(bins = 25)
hist2 = ggplot(data = numeric_data, aes(x =Height)) +
ggtitle("Distribution of Height") + geom_histogram(bins = 25)
hist3 = ggplot(data = numeric_data, aes(x =Body.mass.index)) +
ggtitle("Distribution of Body.mass.index") + geom_histogram(bins = 25)
hist4 = ggplot(data = numeric_data, aes(x =Absenteeism.time.in.hours)) +
ggtitle("Distribution of Absenteeism.time.in.hours") + geom_histogram(bins = 25)
gridExtra::grid.arrange(hist1,hist2,hist3,hist4,ncol=2)
########################################OUTLIER ANALYSIS########################################
#Get the data with only numeric columns
numeric_index = sapply(df, is.numeric)
numeric_index
numeric_data = df[,numeric_index]
#Get the data with only factor columns
factor_data = df[,!numeric_index]
#Check for outliers using boxplots
for(i in 1:ncol(numeric_data)) {
assign(paste0("box",i), ggplot(data = df, aes_string(y = numeric_data[,i])) +
stat_boxplot(geom = "errorbar", width = 0.5) +
geom_boxplot(outlier.colour = "red", fill = "grey", outlier.size = 1) +
labs(y = colnames(numeric_data[i])) +
ggtitle(paste("Boxplot: ",colnames(numeric_data[i]))))
}
#Arrange the plots in grids
gridExtra::grid.arrange(box1,box2,box3,box4,ncol=2)
gridExtra::grid.arrange(box5,box6,box7,box8,ncol=2)
gridExtra::grid.arrange(box9,ncol=1)
#Get the names of numeric columns
numeric_columns = colnames(numeric_data)
#Replace all outlier data with NA
for(i in numeric_columns){
val = df[,i][df[,i] %in% boxplot.stats(df[,i])$out]
print(paste(i,length(val)))
df[,i][df[,i] %in% val] = NA
}
#Check number of missing values
sapply(df,function(x){sum(is.na(x))})
#Get number of missing values after replacing outliers as NA
missing_values_out = data.frame(sapply(df,function(x){sum(is.na(x))}))
missing_values_out$Columns = row.names(missing_values_out)
row.names(missing_values_out) = NULL
names(missing_values_out)[1] = "Miss_perc"
missing_values_out$Miss_perc = ((missing_values_out$Miss_perc/nrow(emp_absent)) *100)
missing_values_out = missing_values_out[,c(2,1)]
missing_values_out = missing_values_out[order(-missing_values_out$Miss_perc),]
missing_values_out
#Compute the NA values using KNN imputation
df = knnImputation(df, k = 5)
#Check if any missing values
sum(is.na(df))
#Check for multicollinearity using corelation graph
library(corrgram)
corrgram(numeric_data, order = NULL, upper.panel=panel.pie,
text.panel=panel.txt, main = "Correlation Plot")
df = subset.data.frame(df, select = -c(Body.mass.index))
#Make a copy of Clean Data
clean_data = df
#Remove dependent variable
numeric_index = sapply(df,is.numeric)
numeric_data = df[,numeric_index]
numeric_columns = names(numeric_data)
numeric_columns = numeric_columns[-9]
#Normalization of continuous variables
for(i in numeric_columns){
print(i)
df[,i] = (df[,i] - min(df[,i]))/
(max(df[,i]) - min(df[,i]))
}
#Get the names of factor variables
factor_columns = names(factor_data)
#Create dummy variables of factor variables
df = dummy.data.frame(df, factor_columns)
rmExcept(keepers = c("df","emp_absent"))
#Splitting data into train and test data
set.seed(1)
train_index = sample(1:nrow(df), 0.8*nrow(df))
str(train_index)
train = df[train_index,]
test = df[-train_index,]
str(train)
prin_comp = prcomp(train)
train_index = sample(1:nrow(df), 0.8*nrow(df))
str(train_index)
train = df[train_index,]
test = df[-train_index,]
prin_comp = prcomp(train)
#Compute standard deviation of each principal component
pr_stdev = prin_comp$sdev
#Compute variance
pr_var = pr_stdev^2
#Proportion of variance explained
prop_var = pr_var/sum(pr_var)
#Cumulative scree plot
plot(cumsum(prop_var), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
type = "b")
#Add a training set with principal components
train.data = data.frame(Absenteeism.time.in.hours = train$Absenteeism.time.in.hours, prin_comp$x)
# From the above plot selecting 45 components since it explains almost 95+ % data variance
train.data =train.data[,1:45]
#Transform test data into PCA
test.data = predict(prin_comp, newdata = test)
test.data = as.data.frame(test.data)
#Select the first 45 components
test.data=test.data[,1:45]
#Build decsion tree using rpart
dt_model = rpart(Absenteeism.time.in.hours ~., data = train.data, method = "anova")
#Predict the test cases
dt_predictions = predict(dt_model,test.data)
#Create data frame for actual and predicted values
df_pred = data.frame("actual"=test[,114], "dt_pred"=dt_predictions)
head(df_pred)
#Calcuate MAE, RMSE, R-sqaured for testing data
print(postResample(pred = dt_predictions, obs = test$Absenteeism.time.in.hours))
#Plot a graph for actual vs predicted values
plot(test$Absenteeism.time.in.hours,type="l",lty=2,col="green")
lines(dt_predictions,col="blue")
#Train the model using training data
rf_model = randomForest(Absenteeism.time.in.hours~., data = train.data, ntrees = 500)
#Predict the test cases
rf_predictions = predict(rf_model,test.data)
#Create dataframe for actual and predicted values
df_pred = cbind(df_pred,rf_predictions)
head(df_pred)
#Calcuate MAE, RMSE, R-sqaured for testing data
print(postResample(pred = rf_predictions, obs = test$Absenteeism.time.in.hours))
#Plot a graph for actual vs predicted values
plot(test$Absenteeism.time.in.hours,type="l",lty=2,col="green")
lines(rf_predictions,col="blue")
#Train the model using training data
lr_model = lm(Absenteeism.time.in.hours ~ ., data = train.data)
#Get the summary of the model
summary(lr_model)
#Predict the test cases
lr_predictions = predict(lr_model,test.data)
#Create dataframe for actual and predicted values
df_pred = cbind(df_pred,lr_predictions)
head(df_pred)
#Calcuate MAE, RMSE, R-sqaured for testing data
print(postResample(pred = lr_predictions, obs =test$Absenteeism.time.in.hours))
#Plot a graph for actual vs predicted values
plot(test$Absenteeism.time.in.hours,type="l",lty=2,col="green")
lines(lr_predictions,col="blue")
